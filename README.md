# Big Data Mining Project: Apache Spark Analysis

This project is a collaborative effort focused on processing and analyzing large-scale datasets using the **Apache Spark** framework.The project demonstrates skills in SQL queries for distributed data, Docker containerization, and team-based development using Git Flow.

## Team Roles & Responsibilities

Based on our internal team agreement, the following roles have been assigned for each technical phase.

### Phase 1: Preparation stage
* **Repository Management (Anastasiia Kuznietsova):** Setting up the Git repository and establishing the branch structure (Main/Develop).
* **Initial Configuration (Ania Savchuk):** Creating the foundational `.gitignore` and `README.md` files.
* **Reporting (Olena Novosad):** Initializing the project report and tracking development progress.
* **Data Sourcing (Anastasiia Zhmud):** Selecting and downloading the dataset to an external project folder.

### Phase 2: Environment Setup stage
* **Docker Configuration (Anastasiia Zhmud):** Creating the `Dockerfile` for the Spark environment setup.
* **Documentation Update (Olena Novosad):** Updating configuration files and technical documentation.
* **System Entry Point (Anastasiia Kuznietsova):** Developing the `main.py` file as the primary application entry point.
* **Verification (Ania Savchuk):** Testing the environment and documenting setup results.

### Phase 3: Extraction stage
* **Schema Design (Anastasiia Kuznietsova):** Defining explicit data schemas for the selected dataset.
* **Data Ingestion (Anastasiia Zhmud):** Implementing functions to read raw data into Spark DataFrames.
* **Data Validation (Ania Savchuk):** Verifying the accuracy and integrity of the ingestion process.
* **Module Development (Olena Novosad):** Organizing operations into a reusable Python module.

### Phase 4: Data Preprocessing stage
* **Statistical Analysis (Olena Novosad):** Generating descriptive statistics for numerical features.
* **Data Typing (Ania Savchuk):** Parsing and converting data into required formats.
* **Feature Engineering (Anastasiia Kuznietsova):** Analyzing and selecting informative features for the analytical models.
* **Data Cleaning (Anastasiia Zhmud):** Handling missing values, duplicates, and noise in the dataset.

### Phase 5: Data Preprocessing stage
* Each member implements 6 unique business questions using filter, join, group by, and window functions

### Phase 6: Recording results stage
* All members participate in exporting their query results into .csv files.

### Phase 7: Presentation stage
* As a team, we provide a holistic demonstration of the data processing pipeline and showcase the specific contribution of each member to the project's success.

---

## Tech Stack
* **Framework:** Apache Spark (PySpark).
* **Database:** Spark SQL.
* **Containerization:** Docker.
* **Version Control:** Git.